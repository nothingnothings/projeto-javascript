In this course section we - of course - discussed JavaScript performance.

When it comes to the overall performance of a website, it's not just JavaScript that matters though. Besides other client-side optimizations (e.g. CSS, images => see: https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency), it's also the server-side where you can improve performance.



Os dados acima capturam a tendência de crescimento do número de bytes baixados em destinos populares na Web entre janeiro de 2013 e janeiro de 2014. Naturalmente, nem todo site cresce com a mesma velocidade ou exige a mesma quantidade de dados. É por isso que destacamos os quartis diferentes na distribuição: 50º (mediana), 75º e 90º.

Um site na mediana, no início de 2014, consistia em 75 solicitações que acumulavam um total de até 1.054 KB de bytes transferidos. O número total de bytes (e solicitações) cresceu com um ritmo constante no ano anterior. Isso não deve ser uma grande surpresa, mas traz implicações de desempenho importantes. Sim, as velocidades da Internet estão aumentando, mas aumentam com taxas diferentes em países diferentes e muitos usuários ainda estão sujeitos e limites de dados e planos limitados de alto custo, particularmente em dispositivos móveis.

Ao contrário de seus equivalentes no desktop, os aplicativos da Web não exigem um processo de instalação separado, basta inserir o URL e começar a usar. Isso é um recurso importante da Web. No entanto, para que isso aconteça, muitas vezes temos de recuperar dezenas ou, algumas vezes, centenas de recursos diversificados, que podem chegar a megabytes de dados e devem ser recebidos juntos em centenas de milissegundos para facilitar a experiência da Web instantânea que todos queremos.

Alcançar essa experiência da Web instantânea, considerando esses requisitos, não é uma tarefa trivial. É por isso que a otimização da eficiência do conteúdo é crítica: eliminação de downloads desnecessários, otimização da codificação de transferência de cada recurso por meio de várias técnicas de compressão e aproveitamento de armazenamento em cache sempre que possível para eliminar downloads redundantes.


----------------------------------------------------------------------------









Next to eliminating unnecessary resource downloads, the best thing you can do to improve page-load speed is to minimize the overall download size by optimizing and compressing the remaining resources.

Data compression 101
After you’ve eliminated any unnecessary resources, the next step is to compress the remaining resources that the browser has to download. Depending on the resource type—text, images, fonts, and so on—there are many different techniques to choose from: generic tools that can be enabled on the server, pre-processing optimizations for specific content types, and resource-specific optimizations that require input from the developer.

Delivering the best performance requires a combination of all of these techniques.

TL;DR
Compression is the process of encoding information using fewer bits.
Eliminating unnecessary data always yields the best results.
There are many different compression techniques and algorithms.
You will need a variety of techniques to achieve the best compression.
The process of reducing the size of data is data compression. Many people have contributed algorithms, techniques, and optimizations to improve compression ratios, speed, and memory requirements of various compressors. A full discussion of data compression is beyond the scope of this topic. However, it's important to understand, at a high level, how compression works and the techniques you can use to reduce the size of various assets that your pages require.

To illustrate the core principles of these techniques, consider the process of optimizing a simple text message format that was invented just for this example:


# Below is a secret message, which consists of a set of headers in
# key-value format followed by a newline and the encrypted message.
format: secret-cipher
date: 08/25/16
AAAZZBBBBEEEMMM EEETTTAAA
Messages may contain arbitrary annotations, which are indicated by the "#" prefix. Annotations do not affect the meaning or any other behavior of the message.
Messages may contain headers, which are key-value pairs (separated by ":") that appear at the beginning at the message.
Messages carry text payloads.
What can you do to reduce the size of the above message, which is currently 200 characters?

The comment is interesting, but it doesn’t actually affect the meaning of the message. Eliminate it when transmitting the message.
There are good techniques to encode headers in an efficient manner. For example, if you know that all messages have "format" and "date," you could convert those to short integer IDs and just send those. However, that might not be true, so just leave it alone for now.
The payload is text only, and while we don’t know what the contents of it really are (apparently, it’s using a "secret-message"), just looking at the text shows that there's a lot of redundancy in it. Perhaps instead of sending repeated letters, you can just count the number of repeated letters and encode them more efficiently. For example, "AAA" becomes "3A", which represents a sequence of three A’s.
Combining these techniques produces the following result:


format: secret-cipher
date: 08/25/16
3A2Z4B3E3M 3E3T3A
The new message is 56 characters long, which means that you've compressed the original message by an impressive 72%.

This is all great, but how does this help us optimize our web pages? We’re not going to try to invent our compression algorithms, but, as you will see, we can use the exact same techniques and thought processes when optimizing various resources on our pages: preprocessing, context-specific optimizations, and different algorithms for different content.








Minification: preprocessing & context-specific optimizations (minification é isso...)










Content-specific optimizations can significantly reduce the size of delivered resources.
Content-specific optimizations are best applied as part of your build/release cycle.
The best way to compress redundant or unnecessary data is to eliminate it altogether. We can’t just delete arbitrary data, but in some contexts where we have content-specific knowledge of the data format and its properties, it's often possible to significantly reduce the size of the payload without affecting its actual meaning.












<html>
  <head>
  <style>
     /* awesome-container is only used on the landing page */
     .awesome-container { font-size: 120% }
     .awesome-container { width: 50% }
  </style>
 </head>

 <body>
   <!-- awesome container content: START -->
    <div>…</div>
   <!-- awesome container content: END -->
   <script>
     awesomeAnalytics(); // beacon conversion metrics
   </script>
 </body>
</html>




Consider the simple HTML page above and the three different content types that it contains: HTML markup, CSS styles, and JavaScript. Each of these content types has different rules for what constitutes valid content, different rules for indicating comments, and so on. How can we reduce the size of this page?

Code comments are a developer’s best friend, but the browser doesn't need to see them! Simply stripping the CSS (/* … */), HTML (<!-- … -->), and JavaScript (// …) comments can significantly reduce the total size of the page.






A "smart" CSS compressor could notice that we’re using an inefficient way of defining rules for ".awesome-container" and collapse the two declarations into one without affecting any other styles, saving more bytes.




Whitespace (spaces and tabs) is a developer convenience in HTML, CSS, and JavaScript. An additional compressor could strip out all the tabs and spaces.







After applying the above steps, the page goes from 406 to 150 characters, a 63% compression savings. Granted, it’s not very readable, but it also doesn’t have to be: you can keep the original page as your "development version" and then apply the steps above whenever you're ready to release the page on your website.

Taking a step back, the above example illustrates an important point: 
a general-purpose compressor—say, one designed to compress arbitrary
 text—could probably do a pretty good job of compressing the page above, 
 but it would never know to strip the comments, collapse the CSS rules, 
 or dozens of other content-specific optimizations. 
 This is why preprocessing/minification/context-aware optimization can be such a powerful tool.




Note: Case in point, the uncompressed development version 
of the jQuery library is now approaching ~300KB. The same library, 
but minified (removed comments, etc.) is about 3x smaller: ~100KB.











Similarly, the techniques described above can be extended beyond just text-based assets. 
Images, video, and other content types all contain their own forms of metadata and various 
payloads. For example, whenever you take a picture with a camera,
 the photo also typically embeds a lot of extra information: camera settings, location, 
 and so on. Depending on your application, this data might be critical (for example, a
  photo-sharing site) or completely useless, and you should consider whether it is worth removing.
   In practice, this metadata can add up to tens of kilobytes for every image.

In short, as a first step in optimizing the efficiency of your assets, 
build an inventory of the different content types and consider what kinds of content-specific
 optimizations you can apply to reduce their size. Then, after you’ve figured out what
  they are, automate these optimizations by adding them to your build and release
   processes to ensure that the optimizations are applied.



-------------------------------------------------------------------------



IMPORTANTE!!! -->   Text compression with GZIP





GZIP performs best on text-based assets: CSS, JavaScript, HTML.
All modern browsers support GZIP compression and will automatically request it.
Your server must be configured to enable GZIP compression.
Some CDNs require special care to ensure that GZIP is enabled.






GZIP is a generic compressor that 
can be applied to any stream of bytes. 
Under the hood, it remembers some of the previously seen content 
and attempts to find and replace duplicate data
 fragments in an efficient way. (If you're curious, here's a great low-level
  explanation of GZIP.) However,
   in practice, GZIP performs best on text-based content, 
   often achieving compression rates of as high as 70-90% for larger files, 
   whereas running GZIP on assets that are already compressed 
   via alternative algorithms (for example, most image formats) yields little to no improvement.





All modern browsers support and automatically 
negotiate GZIP compression for all HTTP requests.
 You must ensure that the server is properly configured to serve the
  compressed resource when the client requests it.








OU SEJA:



BROWSER --> não precisa 'configurar' GZIP



SERVER --> precisa QUE SEUS ARQUIVOS/SEU DATABASE SEJA CONFIGURADO PARA QUE OS ARQUIVOS SEJAM 'SERVIDOS' como gzip...




Library	Size	Compressed size	    Compression ratio
jquery-1.11.0.js	276 KB	82 KB	     70%
jquery-1.11.0.min.js	94 KB	33 KB	65%
angular-1.2.15.js	729 KB	182 KB	      75%
angular-1.2.15.min.js	101 KB	37 KB	63%
bootstrap-3.1.1.css	118 KB	18 KB	     85%
bootstrap-3.1.1.min.css	98 KB	17 KB	83%
foundation-5.css	186 KB	22 KB	     88%
foundation-5.min.css	146 KB	18 KB	88%









The above table shows the savings that GZIP compression
 produces for a few of the most popular JavaScript libraries and CSS frameworks.
  The savings range from 60 to 88%, and the combination of minified files
   (identified by “.min” in their filenames), plus GZIP, offers even more savings.





(então os arquivos ficariam como 

'.gzip.min'?)







It's not an either question, use both, serve the minified file, over a gzip stream to the browser for the best/quickest delivery possibly.




Most web servers and almost every current browser support gzip. 
You're serving the minified file, with internal variables shortened etc...but
 then deliverying a zipped version of that to the client. 
 By doing this you're delivering the minimum amount of javascript for the client 
to execute and delivering the smallest payload...so a quicker download for your user.



Also, remember to set cache headers so the 
client's not re-fetching the file (https://developers.google.com/speed/docs/insights/v5/get-started)...and there are other performance tips to go along with this you should read (https://web.dev/lighthouse-performance/) :)




RESUMINDO:



1) Apply content-specific optimizations first: CSS, JS, and HTML minifiers.






2) Apply GZIP to compress the minified output.





Enabling GZIP is one of the simplest
 and highest-payoff optimizations to implement, 
 and yet, many people don't implement it. Most web servers compress content on your behalf, 
 and you just need to verify that the server is correctly configured to compress all the content types
  that benefit from GZIP compression.






The HTML5 Boilerplate project contains sample configuration files (https://github.com/h5bp/server-configs)  (PROCURAR POR CONFIG FILE DO 'node.js'...) ---> (https://github.com/h5bp/server-configs-node) ---> essa CONFIG FILE já vem com o GZIP ativado.... ---> mas esse código JÁ ESTÁ DEPRECADO (é muito velho, e não está mais apoiado pela comunidade...)
for all the most popular servers with detailed comments for each configuration flag and setting. 
To determine the best configuration for your server, do the following:





A quick and simple way to see GZIP in action is to open Chrome DevTools and inspect the “Size / Content” 
column in the Network panel: “Size” indicates the transfer size of the asset, and “Content” the uncompressed 
size of the asset. For the HTML asset in the preceding example, GZIP saved 98.8 KB during the transfer.  ------> 

É SÓ IR NA ABA 'NETWORK' e passar o mouse em cima de algum dos elementos que foi baixado...


o valor de 'transferred over network' --> É O VALOR DO ARQUIVO/TEXTO __COMPRIMIDO___ PELO GZIP...


já o valor de 'resource size' ---> É O VALOR DESSE MESMO ARQUIVO, MAS DESCOMPRESSO..... (Expandido..)






Note: Sometimes, GZIP increases the size of the asset. 
Typically, this happens when the asset is very small and the overhead of the GZIP dictionary is 
higher than the compression savings, or when the resource is already well compressed. 
To avoid this problem, some servers allow you to specify a minimum filesize threshold.




Finally, while most servers automatically compress 
the assets for you when serving them to the user, some CDNs require extra care and manual
 effort to ensure that the GZIP asset is served. Audit your site and ensure that your assets are, 
 in fact, being compressed.







OK, MAS COMO PODEMOS ATIVAR A OPTIMIZAÇÃO DO 'COMPRESS' DE CÓDIGO DO GZIP no nosso 


SERVIDOR 'node.js'...?



--> https://github.com/expressjs/compression


Compression is about zipping static assets (CSS, JS, images) before serving them. Modern browsers know how to unzip such files and will automatically do so. Since zipped assets are transferred, less data is sent from server to client => Faster load time.

How you set up compression depends on which server/ service you're using. For example on Firebase, static assets are automatically compressed.

When having your own NodeJS server-side code, you would have to manually ensure that static assets are compressed (https://github.com/expressjs/compression).
















---------------------------------------------------------------


IMAGE OPTIMIZATION... ----> https://web.dev/fast/#optimize-your-images


































---------------------------------------------------------------------------



Executing code on the server-side (e.g. NodeJS) is one thing but the server configuration is also important.

Specifically, there are three main areas of improvement which you might want to look into:

Compression of served assets

Caching (client-side and server-side)

HTTP/2

Compression
Compression is about zipping static assets (CSS, JS, images) before serving them. Modern browsers know how to unzip such files and will automatically do so. Since zipped assets are transferred, less data is sent from server to client => Faster load time.

How you set up compression depends on which server/ service you're using. For example on Firebase, static assets are automatically compressed.

When having your own NodeJS server-side code, you would have to manually ensure that static assets are compressed (https://github.com/expressjs/compression).

Caching
Caching is a complex topic - it's about saving data or files for re-use and it can be done on different levels.

For example, browser automatically cache files (e.g. JS files) for you - based on the caching headers set by the serving host (https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control). So controlling these headers on the server-side config, allows you to control how browsers will cache such files. This can help you avoid unnecessary data transfer but of course you also have to make sure that visitors of your site don't miss out on important updates.

Server-side caching is all about storing data you work with on the server (e.g. fetched from a database) such that multiple requests requesting the same data can get that cached data.

You can learn more about caching here: https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/http-caching

And here: https://wp-rocket.me/blog/different-types-of-caching/

HTTP/2
HTTP/2 is the latest "form" of the Http protocol and unlike HTTP 1, it allows for "server push". That means that servers can push required assets/ files actively to a client (instead of waiting for the client to request them).

You can learn more about it here: https://developers.google.com/web/fundamentals/performance/http2